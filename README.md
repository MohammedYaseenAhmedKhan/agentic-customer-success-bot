# Agentic Customer Support Assistant (RAG-based)

A production-style, multi-agent customer support assistant built using
Retrieval-Augmented Generation (RAG), deterministic policy handling,
human escalation, and analytics logging.

The system is designed to operate **even when LLM APIs are unavailable**,
ensuring reliability, auditability, and enterprise readiness.

---

## ğŸš€ Key Features
- Multi-agent architecture (Knowledge, Policy, Escalation, Analytics)
- Retrieval-Augmented Generation (RAG) using FAISS
- Local embeddings with SentenceTransformers
- Optional Gemini LLM integration with safe fallback
- Deterministic policy and compliance handling
- Human-in-the-loop escalation via ticketing
- Analytics logging for observability and audits
- Streamlit-based interactive UI
- Source-aware responses with document citations

---

## ğŸ§  System Architecture

User Query
â†“
Intent Classification
â†“
Agent Router
â†“
+-------------------------------+
| Knowledge Agent (RAG + LLM) |
| Policy Agent (Rules-based) |
| Escalation Agent (Human) |
+-------------------------------+
â†“
Analytics Agent (Logging)


---

## ğŸ§° Tech Stack
- Language: Python
- Embeddings: SentenceTransformers
- Vector Store: FAISS
- LLM: Google Gemini (optional)
- UI: Streamlit
- Document Parsing: pdfplumber, python-docx
- Analytics: JSONL-based logging
- Environment: virtualenv

---

## ğŸ“ Project Structure

agentic-customer-support-bot/
â”œâ”€â”€ app/ # Streamlit UI
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ agents/ # Agent implementations
â”‚ â”œâ”€â”€ rag/ # Retrieval pipeline
â”‚ â”œâ”€â”€ ingestion/ # Knowledge base ingestion
â”‚ â”œâ”€â”€ llm/ # LLM integration layer
â”‚ â””â”€â”€ main.py # Orchestration entry point
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ knowledge_base/
â”‚ â”œâ”€â”€ vector_store/
â”‚ â””â”€â”€ analytics/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## â–¶ï¸ Running the Application

### CLI mode
```bash
python src/main.py
Streamlit UI
python -m streamlit run app/streamlit_app.py
âš ï¸ LLM Availability & Fallback Behavior
Uses Gemini LLM when API quota is available

Automatically falls back to retrieval-only answers when quota is exhausted

No code changes required when LLM access is restored

This ensures graceful degradation in production environments.

ğŸ¯ Example Queries
What is the leave policy?

What happens if an employee violates discipline rules?

I want to talk to HR

ğŸ“Š Analytics & Observability
All queries, intents, and routing decisions are logged to:

data/analytics/query_logs.jsonl
This enables:

Identification of common user issues

Knowledge base gap analysis

Agent usage distribution monitoring

Audit and compliance reviews

ğŸ§ª Key Learnings
Designing reliable agent routing with deterministic fallbacks

Combining rule-based systems with LLM-driven agents

Handling LLM unavailability gracefully in production

Building observable and auditable AI systems

Structuring multi-agent architectures for extensibility

ğŸ Project Highlights
LLM-optional, production-safe design

Enterprise-focused compliance handling

Human escalation support

Clean, modular, and extensible architecture
